# åŒ»ç–—æ–‡æœ¬å¤šæ ‡ç­¾åˆ†ç±»ç³»ç»Ÿ

æœ¬é¡¹ç›®åŸºäº BERT æ¨¡å‹å®ç°åŒ»ç–—é—®è¯Šæ–‡æœ¬çš„è‡ªåŠ¨æ ‡ç­¾é¢„æµ‹ï¼Œæ”¯æŒ Top-K æ¨èä¸å±‚çº§åŒ–æ ‡ç­¾ä½“ç³»ã€‚

## ğŸ“¦ æ¨¡å‹é€‰æ‹©

- **åŸºç¡€æ¨¡å‹**ï¼š`chinese-bert-wwm-ext`  
  - æ¥æºï¼š[Hugging Face Model Hub](https://huggingface.co/hfl/chinese-bert-wwm-ext)
  - ç‰¹ç‚¹ï¼šå…¨è¯æ©ç ï¼ˆWhole Word Maskingï¼‰ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨ä¸­æ–‡ NLP ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚
- **ä»»åŠ¡ç±»å‹**ï¼šå¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»ï¼ˆMulti-label Classificationï¼‰
- **å¾®è°ƒæ–¹å¼**ï¼šåœ¨åŒ»ç–—é—®è¯Šæ•°æ®ä¸Šè¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒ
- **è¾“å‡ºå±‚**ï¼šSigmoid æ¿€æ´» + äºŒå…ƒäº¤å‰ç†µæŸå¤±ï¼ˆBCEWithLogitsLossï¼‰

> æ¨¡å‹æƒé‡ç›®å‰åœ¨æœ¬åœ°ï¼Œåç»­ä¸Šä¼ è‡³ Hugging Face Hubï¼š  

## ğŸ—ƒï¸ æ•°æ®é›†

åŸå§‹æ•°æ®æ¥è‡ª Hugging Face å…¬å¼€åŒ»ç–—æ•°æ®é›†ï¼š

- **æ•°æ®æ¥æº**ï¼š[shibing624/medical](https://huggingface.co/datasets/shibing624/medical)
- **å…·ä½“æ–‡ä»¶**ï¼š[`train_zh_0.json`](https://huggingface.co/datasets/shibing624/medical/blob/main/finetune/train_zh_0.json)
- **æ•°æ®å†…å®¹**ï¼šä¸­æ–‡åŒ»ç–—é—®è¯Šå¯¹è¯ï¼ŒåŒ…å«æ‚£è€…æè¿°ä¸åŒ»ç”Ÿè¯Šæ–­æ ‡ç­¾
- **å¤„ç†æµç¨‹**ï¼š
  1. æå– `cleaned.full_text` ä½œä¸ºè¾“å…¥æ–‡æœ¬
  2. ç­›é€‰é«˜é¢‘ Top-50 æ ‡ç­¾ä½œä¸ºå€™é€‰æ ‡ç­¾é›†ï¼ˆè§ `config.py`ï¼‰
  3. æŒ‰ 8:1:1 åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†

## å¿«é€Ÿä½¿ç”¨

### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ¨èï¼‰

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_name = "yczSEU/med-chat-user-label"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

text = "æˆ‘ç©ºè…¹è¡€ç³–12ï¼Œæ˜¯ä¸æ˜¯å¾—äº†ç³–å°¿ç—…ï¼Ÿæœ€è¿‘è¿˜å£æ¸´ï¼Œæ€»æƒ³ä¸Šå•æ‰€ã€‚"
inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
with torch.no_grad():
    logits = model(**inputs).logits
    probs = torch.sigmoid(logits)

# è·å– Top-3 é¢„æµ‹
topk = torch.topk(probs[0], k=3)
for score, idx in zip(topk.values, topk.indices):
    print(f"æ ‡ç­¾: {model.config.id2label[idx.item()]}, ç½®ä¿¡åº¦: {score:.4f}")
